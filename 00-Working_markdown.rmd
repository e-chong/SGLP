---
title: "SGLP Working Draft"
author: "Eugene Chong"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
    toc_depth: 5
---

```{r setup, include=FALSE, echo = FALSE}
library(knitr)
cache_state = TRUE
eval_state = FALSE # use this for testing chunks that should/should not be run in the final knit doc

opts_chunk$set(echo = TRUE, cache = cache_state, message = FALSE)
```

# Admin

Run scripts and load objects

```{r packages, warning=FALSE}
source("~scripts/00 - Admin.R") 
source("~scripts/01 - Utility Functions.R")
source("~scripts/10 - Read Siegel Data.R")
guns_df <- readRDS("~outputs/10/11_guns_df.rds")
guns_clean <- readRDS("~outputs/20/21_guns_clean.rds")
guns_list <- readRDS("~outputs/20/21_guns_list.rds")
siegelSum <- readRDS("~outputs/20/20_siegelSum.RDS")
guns_list_shp <- readRDS("~outputs/20/21_guns_list_shp.rds")
guns_list_shp_byYear <- readRDS("~outputs/20/21_guns_list_shp_byYear.rds")
source("~scripts/31 - Explore crime data.R")
tracts_crimeCounts <- readRDS("~outputs/30/33_tracts_crimeCounts.rds")
BGs_crimeCounts <- readRDS("~outputs/30/33_BGs_crimeCounts.rds")
BG_selection_list <- readRDS("~outputs/20/22_BG_selection_list.rds")
tract_selection_list <- readRDS("~outputs/20/22_tract_selection_list.rds")
```

# Read Data

## Siegel data for all cities

```{r}
# source("~scripts/10 - Read Siegel Data.R")
```

## Gun incidents for all cities

```{r, eval = eval_state}
# source("~scripts/11 - Read crime data.R")
# guns_df <- readRDS("~outputs/10/11_guns_df.rds")
```

```{r}
# source("~scripts/21 - Clean crime data.R")
# guns_clean <- readRDS("~outputs/20/21_guns_clean.rds")
# guns_list <- readRDS("~outputs/20/21_guns_list.rds")
```

Sample of data for testing

```{r, eval = eval_state}
set.seed(1)
guns_sample_df <- sample_n(guns_df, 10000) 
guns_sample_ls <- guns_sample_df %>% 
  split(., # split into nested list of dfs by city
        f = .$city)
```

## Census data for all cities

```{r, results = "hide"}
# source("~scripts/12 - Read census data.R")
```

# Explore Data

## Siegel

Date range: 1991-2019

```{r}
range(siegel_raw$year)
```

134 different laws...

```{r}
unique(siegel_raw$law)
```

Organized into 14 categories...

```{r}
unique(siegel_raw$Category)
```

And 50 sub-categories

```{r}
unique(siegel_raw$Sub.Category)
```

### Scores over time 

```{r}
# source("~scripts/20 - Clean Siegel data.R")
# siegelSum <- readRDS("~outputs/20/20_siegelSum.RDS")
```

```{r, fig.height = 20, fig.width = 13}
ggplot(siegelSum,
       aes(x = year,
           y = score)) +
  geom_line(size = 1) +
  # geom_point() +
  facet_wrap(~ state, ncol = 5, scales = "free_x") +
  plotTheme() +
  scale_x_continuous(breaks = seq(min(siegelSum$year), max(siegelSum$year), 5)) +
  labs(title = "Siegel Scores",
       x = "Year",
       y = "Score (sum of gun laws)") +
  theme(panel.spacing.x = unit(8, "mm"))
```

## Gun Crimes

Number of Cities: 34
Number of States: 29

```{r}
unique(guns_clean$city)
unique(guns_clean$state)
```

**Date Range:**

* Occurrence Date: 06/19/1922 - 05/01/2020
* Report Date: 10/03/1960 - 05/01/2020

```{r, eval = eval_state}
plan(multiprocess)
guns_sample_ls <- future_map(guns_sample_ls,
                        ~ .x %>% 
                          mutate(clean_occur_date = anydate(occurdate), # use built-in formats from anytime package
         # correct some incorrectly parsed observations
         clean_occur_date = case_when(occurdate == "1" ~ as.Date(NA),
                                      clean_occur_date < as.Date("1900-01-01") ~ as.Date(NA), 
                                      is.na(clean_occur_date) & 
                                        str_detect(occurdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(occurdate, "%m/%d/%y"),
                                      TRUE ~ clean_occur_date),
         clean_report_date = anydate(reportdate),
         clean_report_date = case_when(reportdate == "1" ~ as.Date(NA),
                                       clean_report_date < as.Date("1900-01-01") ~ as.Date(NA),
                                       is.na(clean_report_date) & 
                                        str_detect(reportdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(reportdate, "%m/%d/%y"),
                                      TRUE ~ clean_report_date)))
```

```{r}
range(guns_clean$clean_occur_date, na.rm = TRUE)
range(guns_clean$clean_report_date, na.rm = TRUE)
```

Benchmarking results show `furrr` is fastest for manipulating the data.
```{r, eval = eval_state}
benchmark_results <- benchmark("flat" = {
  flat <- guns_sample_df %>% 
  mutate(clean_occur_date = anydate(occurdate), # use built-in formats from anytime package
         # correct some incorrectly parsed observations
         clean_occur_date = case_when(occurdate == "1" ~ as.Date(NA),
                                      is.na(clean_occur_date) & 
                                        str_detect(occurdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(occurdate, "%m/%d/%y"),
                                      TRUE ~ clean_occur_date),
         clean_report_date = anydate(reportdate),
         clean_report_date = case_when(is.na(clean_report_date) & 
                                        str_detect(reportdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(reportdate, "%m/%d/%y"),
                                      TRUE ~ clean_report_date))}, # benchmarking shows the furrr approach is fastest (but still not very fast)

"purrr" = {map(guns_sample_ls,
                        ~ .x %>% 
                          mutate(clean_occur_date = anydate(occurdate), # use built-in formats from anytime package
         # correct some incorrectly parsed observations
         clean_occur_date = case_when(occurdate == "1" ~ as.Date(NA),
                                      is.na(clean_occur_date) & 
                                        str_detect(occurdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(occurdate, "%m/%d/%y"),
                                      TRUE ~ clean_occur_date),
         clean_report_date = anydate(reportdate),
         clean_report_date = case_when(is.na(clean_report_date) & 
                                        str_detect(reportdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(reportdate, "%m/%d/%y"),
                                      TRUE ~ clean_report_date)))},

# plan(multiprocess)
"furrr" = {future_map(guns_sample_ls,
                        ~ .x %>% 
                          mutate(clean_occur_date = anydate(occurdate), # use built-in formats from anytime package
         # correct some incorrectly parsed observations
         clean_occur_date = case_when(occurdate == "1" ~ as.Date(NA),
                                      is.na(clean_occur_date) & 
                                        str_detect(occurdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(occurdate, "%m/%d/%y"),
                                      TRUE ~ clean_occur_date),
         clean_report_date = anydate(reportdate),
         clean_report_date = case_when(is.na(clean_report_date) & 
                                        str_detect(reportdate,
                                                   ".*\\d+/\\d+/\\d+.*") ~ # e.g "12/3/15", "12/3/15 1600" 
                                        as.Date(reportdate, "%m/%d/%y"),
                                      TRUE ~ clean_report_date)))},
  replications = 1
)
```

### Plots

Data:

```{r}
# source("~scripts/21 - Clean crime data.R")
# guns_list_shp <- readRDS("~outputs/20/21_guns_list_shp.rds")
```

Source script: 

```{r}
# source("~scripts/31 - Explore crime data.R")
```

Crime counts by city

```{r}
gunIncident_summary %>% 
  arrange(desc(prop)) %>% 
  kable() %>% 
  kable_styling()
```

```{r, fig.height=6}
gunIncidentsByCityPlot <- readRDS("~outputs/30/31c_gunIncidentsByCityPlot.rds")
gunIncidentsByCityPlot
```

How many NA observations?

```{r}
na_coords_summary <- map(guns_list,
                            ~ sum(is.na(.x$lon) | is.na(.x$lat)) /
                              nrow(.x)) %>% 
  bind_rows() %>% 
  gather(key = "City",
         value = "pct_NA")
```

```{r}
na_coords_summary %>% 
  arrange(desc(pct_NA)) %>% 
  kable() %>% 
  kable_styling()
```

#### All Cities

##### Maps of each city  {.tabset}

CHANGE THIS TO QUADRAT ANALYSIS MAPS

```{r, results='asis', echo = FALSE}
# source("~scripts/31a - Map all crimes.R") 

tmp <- list.files("~outputs/Plots/31a_gunCrimes", 
                  full.names = TRUE) 

for (i in 1:length(tmp)) {
  cat("###### ",names(guns_list[i]),"\n")
  cat(paste0("![](", tmp[i], "){width=65%}"), "\n")
  cat('\n\n') 
}
```


### Moran's I

**What is Moran's I?**

Inferential statistic ranging from -1 to 1 that describes the level of dispersal/clustering evident in spatial data. Associated with a p-value that provides the statistical significance of the estimate.

![Moran's I range](https://geobitz.files.wordpress.com/2016/01/morans_i_visual.png?w=471&h=250)

That assumes a uniform intensity to all values. We need to choose some value for determining an "intensity" for the areas. For now, I went with "gun crimes per 100 people". Is there another metric that might be better?

Read in the study area geographies w/ crimes per 100, raw crime counts, and population per tract.

Another question is the geographic scope we look at. Some choices. I looked at Census-designated place for now, because those generally align with published city borders.

A concave or convex hull may better fit the data, though.

* Tracts selected via:
    +  Concave hull of gun crime observations
    +  Convex hull of gun crime observations
    +  All tracts in any county with at least 1 gun crime observation
    +  Census-designated places matching the city/county name
* Block Groups selected via the same criteria

```{r}
# source("~scripts/33 - Aggregate crimes and geographies.R") 
# tracts_crimeCounts <- readRDS("~outputs/30/33_tracts_crimeCounts.rds")
# BGs_crimeCounts <- readRDS("~outputs/30/33_BGs_crimeCounts.rds")
```

```{r, cache = FALSE, eval = FALSE}
tmap::tmap_mode("view")

tmap::qtm(tracts_crimeCounts$byCaveHull$`San Francisco`, title = "Concave hull of crimes")
```

```{r, cache = FALSE, eval = FALSE}
tmap::qtm(tracts_crimeCounts$byVexHull$`San Francisco`, title = "Convex hull of crimes")
```

```{r, cache = FALSE, eval = FALSE}
tmap::qtm(tracts_crimeCounts$byCounty$`San Francisco`, title = "County")
```

```{r, cache = FALSE, eval = FALSE}
tmap::qtm(tracts_crimeCounts$byPlace$`San Francisco`, title = "Census-designated place")
```

```{r}
# source("~scripts/34 - Calculate Moran's I.R")  
tracts_I <- readRDS("~outputs/30/34_tracts_I.rds")
BGs_I <- readRDS("~outputs/30/34_BGs_I.rds")
tracts_pop_I <- readRDS("~outputs/30/34_tracts_pop_I.rds")
BGs_pop_I <- readRDS("~outputs/30/34_BGs_pop_I.rds")
tracts_per100_I <- readRDS("~outputs/30/34_tracts_per100_I.rds")
BGs_per100_I <- readRDS("~outputs/30/34_BGs_per100_I.rds")
```

```{r}
# crime Moran's I tract
I_crime_tr <- map_dfr(tracts_I$byPlace,
                     ~ .x$estimate[1],
                     .id = "City") %>% 
  rename(crime_I = `Moran I statistic`)

p_crime_tr <- map_dfr(tracts_I$byPlace,
                     ~ data.frame(pval_crime = .x$p.value),
                     .id = "City") %>% 
  mutate(geo = "Tract",
         pval_crime = ifelse(pval_crime < 0.01, "< 0.01", "> 0.01"))

# crime Moran's I block group
I_crime_BG <- map_dfr(BGs_I$byPlace,
                     ~ .x$estimate[1],
                     .id = "City") %>% 
  rename(crime_I = `Moran I statistic`)

p_crime_BG <- map_dfr(BGs_I$byPlace,
                     ~ data.frame(pval_crime = .x$p.value),
                     .id = "City") %>% 
  mutate(geo = "Block Group",
         pval_crime = ifelse(pval_crime < 0.01, "< 0.01", "> 0.01"))

# pop Moran's I tract
I_pop_tr <- map_dfr(tracts_pop_I$byPlace,
                     ~ .x$estimate[1],
                     .id = "City") %>% 
  rename(pop_I = `Moran I statistic`)

p_pop_tr <- map_dfr(tracts_pop_I$byPlace,
                     ~ data.frame(pval_pop = .x$p.value),
                     .id = "City") %>% 
  mutate(geo = "Tract",
         pval_pop = ifelse(pval_pop < 0.01, "< 0.01", "> 0.01"))

# pop Moran's I block group
I_pop_BG <- map_dfr(BGs_pop_I$byPlace,
                     ~ .x$estimate[1],
                     .id = "City") %>% 
  rename(pop_I = `Moran I statistic`)

p_pop_BG <- map_dfr(BGs_pop_I$byPlace,
                     ~ data.frame(pval_pop = .x$p.value),
                     .id = "City") %>% 
  mutate(geo = "Block Group",
         pval_pop = ifelse(pval_pop < 0.01, "< 0.01", "> 0.01"))

# per100 Moran's I tract
I_per100_tr <- map_dfr(tracts_per100_I$byPlace,
                     ~ .x$estimate[1],
                     .id = "City") %>% 
  rename(per100_I = `Moran I statistic`)

p_per100_tr <- map_dfr(tracts_per100_I$byPlace,
                     ~ data.frame(pval_per100 = .x$p.value),
                     .id = "City") %>% 
  mutate(geo = "Tract",
         pval_per100 = ifelse(pval_per100 < 0.01, "< 0.01", "> 0.01"))

# per100 Moran's I block group
I_per100_BG <- map_dfr(BGs_per100_I$byPlace,
                     ~ .x$estimate[1],
                     .id = "City") %>% 
  rename(per100_I = `Moran I statistic`)

p_per100_BG <- map_dfr(BGs_per100_I$byPlace,
                     ~ data.frame(pval_per100 = .x$p.value),
                     .id = "City") %>% 
  mutate(geo = "Block Group",
         pval_per100 = ifelse(pval_per100 < 0.01, "< 0.01", "> 0.01"))



I_crime_tmp <- left_join(I_crime_tr, p_crime_tr,
                   by = "City") %>% 
  rbind(left_join(I_crime_BG, p_crime_BG, 
                  by = "City"))
I_pop_tmp <- left_join(I_pop_tr, p_pop_tr,
                  by = "City") %>% 
  rbind(left_join(I_pop_BG, p_pop_BG,
                  by = "City"))
I_per100_tmp <- left_join(I_per100_tr, p_per100_tr,
                  by = "City") %>% 
  rbind(left_join(I_per100_BG, p_per100_BG,
                  by = "City"))
I_tmp <- left_join(I_crime_tmp, I_pop_tmp, by = c("City", "geo")) %>% 
  left_join(I_per100_tmp, by = c("City", "geo"))

I_wide <- I_tmp %>% 
  pivot_wider(names_from = "geo",
              values_from = c("crime_I", "pop_I", "pval_crime", "pval_pop", "per100_I", "pval_per100")) %>% 
  dplyr::select(City, 
                per100_tr = per100_I_Tract,
                per100_tr_p = pval_per100_Tract,
                crime_tr = crime_I_Tract,
                crime_tr_p = pval_crime_Tract,
                pop_tr = pop_I_Tract,
                pop_tr_p = pval_pop_Tract,
                per100_BG = `per100_I_Block Group`,
                per100_BG_p = `pval_per100_Block Group`,
                crime_BG = `crime_I_Block Group`,
                crime_BG_p = `pval_crime_Block Group`,
                pop_BG = `pop_I_Block Group`,
                pop_BG_p = `pval_pop_Block Group`) 
```

Below is a table showing Moran's I for crimes per 100 for census tracts and block groups for each study area _selected by the relevant Census-designated place_, raw crime counts, and population. We see a big range from 0 (essentially random gun crime distribution relative to population) to very high 

Questions: 

* Weighting by population is important. Is Moran's I of crimes per 100 the best way?
* How might we interpret a city that has a large difference in Moran's I between tracts and block groups (e.g. Kansas City)?

```{r}
I_wide %>%  
  mutate(crime_tr_p = cell_spec(crime_tr_p,
                           "html",
                           background = ifelse(str_detect(crime_tr_p, ">"),
                             "red", 
                             "white")),
         pop_tr_p = cell_spec(pop_tr_p,
                           "html",
                           background = ifelse(str_detect(pop_tr_p, ">"),
                             "red", 
                             "white")),
         per100_tr_p = cell_spec(per100_tr_p,
                           "html",
                           background = ifelse(str_detect(per100_tr_p, ">"),
                             "red", 
                             "white")),
         crime_BG_p = cell_spec(crime_BG_p,
                           "html",
                           background = ifelse(str_detect(crime_BG_p, ">"),
                             "red", 
                             "white")),
         pop_BG_p = cell_spec(pop_BG_p,
                           "html",
                           background = ifelse(str_detect(pop_BG_p, ">"),
                             "red", 
                             "white")),
         per100_BG_p = cell_spec(per100_BG_p,
                           "html",
                           background = ifelse(str_detect(per100_BG_p, ">"),
                             "red", 
                             "white"))) %>% 
  arrange(desc(per100_tr)) %>%
  kable(format = "html",
        escape = FALSE,
        digits = 2,
        caption = "Moran's I for Crimes per 100 people, Crime Count, and Population by Tract and Block Group") %>%
  kable_styling(bootstrap_options = "striped")
```


##### Crimes by block group for each city {.tabset}

```{r, results='asis', echo = FALSE}
# source("~scripts/33a - Map census geographies by crime counts.R") 

tmp <- list.files("~outputs/Plots/33a_BG_per100_maps/byCaveHull",
                  full.names = TRUE) 

for (i in 1:length(tmp)) { 
  cat("###### ",names(guns_list[i]),"\n")
  cat(paste0("![](", tmp[i], "){width=65%}"), "\n")
  cat('\n\n')
}
```

##### Siegel Scores, Gun Crime Count, and Moran's I by Year {.tabset}

The plots below show the Moran's I (extent of spatial clustering), count of gun crimes, percentage of all crimes that are gun crimes, and Siegel Score for every city by year. The x-axis is aligned on each, so you can read straight down, but some of the cities with fewer years of data will look compressed horizontally.

A note on **outliers:** I noticed last time that several cities (NYC, LA, SF, others) had Moran's Is of around 0, and this persisted even when looking at them year-by-year, despite visual evidence in the maps showing clear clustering (see map of San Francisco above, for example). This was due to outliers in the gun crimes / 100 stat (a few gun crimes in an area with low population lead to extremely high values). So, I filtered out the block groups every year that had values higher than the 99.5th percentile in each city. In LA, this had the effect of changing the Moran's I from 0 to around 0.55, a much more sensible value. The 99.5th percentile cut-off is arbitrary. Also, those block groups were filtered out rather than imputed, so they would be empty on a map.

* **Boston:** Total gun crimes and clustering dropped over the last decade.

* **Chicago:** clustering is steady over time, while total gun crimes have decreased. Siegel score has steadily risen since 1991.

* **Denver:** Gun crimes and clustering seem to have increased. Crime data available from 2014.

* **Detroit:** Decrease in gun crimes and clustering. Steady Siegel Score.

* **Hartford:** Clustering has dipped, while both annual gun crimes and Siegel Score have risen.

* **Indianapolis:** Gradual decline in clustering. Sudden drop in gun crimes (reporting change?). Little change to Siegel Score.

* **Los Angeles:** Rising Siegel Score, little change to overall crimes or clustering.

* **Louisville:** Sharp increase in crimes, little change to clustering and Siegel.

* **Minneapolis:** Decrease in clustering, increase in crimes and Siegel Score.

* **Nashville:** Decrease in clustering, increase in crimes.

* **New York:** Decrease in crimes and increase in Siegel Score. Not change to clustering.

* **Philly:** Decrease in crimes, maybe a slight increase in clustering, little change to Siegel Score.

* **Portland:** Limited sample, but all three seem to have risen recently.

* **San Francisco:** Increase in Siegel Score, but not much change to total crimes or clustering.

* **St. Louis County:** Decrease in Siegel Score and increase in crimes. 

* **Tucson:** Decrease in crimes and clustering.

```{r,  results='asis', echo = FALSE}
# source("~scripts/34a -  Moran's I plots.R") 

tmp <- list.files("~outputs/Plots/34a_siegel_gunCount_MoransI_plots", 
                  full.names = TRUE) 

for (i in 1:length(tmp)) {
  cat("###### ", names(guns_list[i]),"\n")
  cat(paste0("![](", tmp[i], "){width=75%}"), "\n")
  cat('\n\n')
}
```

##### Cluster / Outlier maps {.tabset}

These maps show high- and low-crime clusters (crimes per 100), as well as areas with high- or low-crime compared to their neighbors. Areas in white not shown to have a statistically significant relationship with their neighbors.

**Notes:**

* I calculated this for every year as well, so we can see how clusters move, but not sure how to visualize it. A gif, maybe?

* Next step is to compare demographics across the categories, or at least the hotspots vs. the whole city.

* Intuitively, these maps make sense. At least for the cities I know, the hot spot locations are not surprising.

* Generally, there are far more hotspots than there are cold spots. Makes sense, a long right tail for gun crimes per capita.

* Some edge effects, it seems, particularly with the "low crime" areas. See Baltimore and Chicago for example. For spatial weights calculation, weights are standardized over all links to the block group, so block groups on the edge naturally have fewer neighbors that are weighed more highly.

* Very few "outliers", where the block group is very different from their neighbor.


```{r, results='asis', echo = FALSE}
# source("~scripts/34a -  Moran's I plots.R") 

tmp <- list.files("~outputs/Plots/34a_BG_cluster_maps",
                  full.names = TRUE) 

for (i in 1:length(tmp)) { 
  cat("###### ", names(guns_list[i]),"\n") 
  cat(paste0("![](", tmp[i], "){width=100%}"), "\n")
  cat('\n\n') 
}
```

#### Hotspot demographic and socio-economic variables

```{r}
BGs_per100_localI_census <- readRDS("~outputs/30/34_BGs_per100_localI_census.rds")

BGs_per100_localI_census_tmp <- map(
  BGs_per100_localI_census,
  ~ .x %>% 
    st_drop_geometry() %>% 
    mutate(hotspot = ifelse(str_detect(cluster, "high"),
                            "Yes",
                            "No"),
           majorityMinority_tmp = ifelse(majorityMinority == "Yes",
                                         1,
                                         0)) %>% 
    group_by(hotspot) %>% 
    summarize(Hotspots = n(),
              MdHHInc_weighted = round(weighted.mean(MdHHInc, TotPop, na.rm = TRUE), 0),
              Age_weighted = round(weighted.mean(MdAge, TotPop, na.rm = TRUE), 1),
              White_pct_weighted = round(weighted.mean(White_pct, TotPop, na.rm = TRUE), 2),
              Black_pct_weighted = round(weighted.mean(Black_pct, TotPop, na.rm = TRUE), 2),
              TotPop = sum(TotPop, na.rm = TRUE),
              .groups = "drop") %>% 
    arrange(desc(hotspot))) 


```

```{r} 

num_rows <- map(1:length(names(BGs_per100_localI_census_tmp)), 
                ~ nrow(BGs_per100_localI_census_tmp[[.x]])) %>% 
                  unlist() 

BGs_per100_localI_census_tmp %>%
  bind_rows() %>% 
  # arrange(Species)  %>%  # same order as table results  
  # select(-Species)  %>%
  kable("html",
        caption = "Weighted averages by Moran's I hotspots",
        # align = "",
        col.names = c("Hotspot?",
                      "# block groups",
                      "Med. HH Inc.",
                      "Med. Age",
                      "White %",
                      "Black %",
                      "Total Pop.")
        ) %>%
  kable_styling(full_width = F,
                fixed_thead = T) %>%
  group_rows(index = setNames(num_rows, names(BGs_per100_localI_census_tmp)),
             label_row_css = "background-color: #666; color: #fff;") 


```

##### Change in crimes over time {.tabset}

```{r,  results='asis', echo = FALSE}
# source("~scripts/34a -  Moran's I plots.R") 

tmp <- list.files("~outputs/Plots/33a_BG_per100_byYear_maps/~gifs",
                  full.names = TRUE) 
tmp2 <- list.files("~outputs/Plots/35a_BG_crimeChange_maps",
                  full.names = TRUE) 

for (i in 1:length(tmp)) { 
  cat("###### ", names(guns_list[i]),"\n") 
  # cat(paste0("![](", tmp[i], "){width=100%}"))
  cat(paste0("![](", tmp2[i], "){width=80%}"), "\n")
  cat('\n\n') 
}
```

#### Rise in gun crimes

```{r}
BGs_crimeChange <- readRDS("~outputs/30/35_BGs_crimeChange.rds")

BGs_crimeChange_tmp <- map2(
  BGs_crimeChange,
  BGs_per100_localI_census,
  ~ .x %>% 
    left_join(.y,
              by = "GEOID") %>% 
    # st_drop_geometry() %>% 
    mutate(increase = ifelse(str_detect(CrimeIncrease, "No"),
                            "No",
                            CrimeIncrease),
           majorityMinority_tmp = ifelse(majorityMinority == "Yes",
                                         1,
                                         0)) %>% 
    group_by(increase) %>% 
    summarize(Increases = n(),
              MdHHInc_weighted = round(weighted.mean(MdHHInc, TotPop, na.rm = TRUE), 0),
              Age_weighted = round(weighted.mean(MdAge, TotPop, na.rm = TRUE), 1),
              White_pct_weighted = round(weighted.mean(White_pct, TotPop, na.rm = TRUE), 2),
              Black_pct_weighted = round(weighted.mean(Black_pct, TotPop, na.rm = TRUE), 2),
              TotPop = sum(TotPop, na.rm = TRUE),
              .groups = "drop") %>% 
    arrange(desc(increase)))


```

```{r}

num_rows <- map(1:length(names(BGs_crimeChange_tmp)), 
                ~ nrow(BGs_crimeChange_tmp[[.x]])) %>% 
                  unlist()

BGs_crimeChange_tmp %>%
  bind_rows() %>% 
  # arrange(Species)  %>%  # same order as table results  
  # select(-Species)  %>%
  kable("html",
        caption = "Block groups where gun crimes increased",
        align = "l",
        col.names = c("Increase in gun crimes?",
                      "# block groups",
                      "Med. HH Inc.",
                      "Med. Age",
                      "White %",
                      "Black %",
                      "Total Pop.")
        ) %>%
  kable_styling(full_width = F) %>%
  group_rows(index = setNames(num_rows, names(BGs_per100_localI_census_tmp)),
             label_row_css = "background-color: #666; color: #fff;") 


```


# Spatial Scale Analysis and Selection

**FILL IN LATER**

To this point, we've aggregated gun crimes to the census tracts and block group level, because they are readily available for each city and are associated with demographic/socio-economic data that will provide useful context for analyzing gun violence rates. However, they have the disadvantage of 1) varying considerably in size and 2) not really reflecting the underlying spatial phenomenon. Maybe gun crimes sort into block-group-sized areas, but it'd be advantageous to measure it at many different levels, ranging from the individual block or street up to entire neighborhoods or larger. Ideally, we'd choose a spatial unit of aggregation that was small enough to reflect the distribution of the underlying gun crime data while being large enough to fit with other existing datasets without requiring too much areal interpolation. Additionally, we would expect that the size of the spatial unit would vary across cities, which vary in both the distribution and reporting quality of their crime data.

One method proposed for selecting the "most appropriate" spatial scale comes from this paper: [Identifying the appropriate spatial resolution for the analysis of crime patterns, 2019](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0218324).

The authors define the most appropriate scale as follows:

> For these reasons, choosing the most appropriate spatial unit is very important. As [Oberwittler D, Wikström POH] (p 39) put it: “smaller is generally better”, but spatial units that are too small are problematic also. So what is an ‘appropriate’ spatial scale? Here, we define the most appropriate scale as that which is as large as possible without causing the underlying spatial units to become heterogeneous with respect to the phenomena under study. The appropriate scale, therefore, balances the theoretical and methodological benefits of working with small units against the difficulty in obtaining small-scale data and the small number problems that can emerge. Here the focus is on crime patterns, specifically, but the proposed method has applicability to other spatial phenomena.

They then propose a method for determining the appropriate scale. First, they select two sets of point data driven by the same spatial process (in their case, two consecutive years of data for certain types of crimes in Vancouver). Then, they iteratively overlay sets of grid cells on their study area with progressively smaller sizes. With each iteration, they count the number of crimes from the two data sets that occurred in the grid cells and compare the similarity of spatial distribution/clustering between them using a measure called [Andresen's S](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0218324.s002&type=supplementary). Finally, they plot the number of cells on the x-axis and _S_ on the y-axis. They suggest choosing the number of grid cells with the highest S, and, in the case _S_ plateaus rather than showing a single peak, choosing the number of grid cells at the elbow (much like when choosing a _K_ for _K-means_ cluster analysis). 

`r include_graphics("https://journals.plos.org/plosone/article/figure/image?size=large&id=10.1371/journal.pone.0218324.g008")`






